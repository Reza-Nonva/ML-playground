{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Compressing Data via Dimensionality Reduction\n",
    "This notebook will explore two fundamental techniques that help summarize the information content of a dataset by transforming it into a new feature subspace with lower dimensionality than the original. Specifically, we will focus on **Principal Component Analysis (PCA)** and **Linear Discriminant Analysis (LDA)** for linear dimensionality reduction, and **t-Distributed Stochastic Neighbor Embedding (t-SNE)** for nonlinear dimensionality reduction.\n",
    "\n",
    "# Topics Covered\n",
    "1. **Introduction to Dimensionality Reduction**\n",
    "\n",
    "2. **Principal Component Analysis (PCA)**\n",
    "   - Explanation of how PCA works.\n",
    "   - Extracting the principal components step by step\n",
    "   - Application of PCA on a sample dataset.\n",
    "   - Visualization of results in the reduced feature space.\n",
    "\n",
    "3. **Linear Discriminant Analysis (LDA)**\n",
    "   - Understanding LDA and its connection to classification tasks.\n",
    "   - Application of LDA on a sample dataset.\n",
    "   - Visualization of LDA-transformed data.\n",
    "\n",
    "4. **t-Distributed Stochastic Neighbor Embedding (t-SNE)**\n",
    "   - Overview of t-SNE for nonlinear dimensionality reduction.\n",
    "   - Application of t-SNE for visualizing high-dimensional data.\n",
    "   - Discussion of t-SNE’s strengths and limitations.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ffebf18cd6b9191"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.Introduction to Dimensionality Reduction\n",
    "\n",
    "Overview of the need for dimensionality reduction\n",
    "In modern datasets, it is common to encounter high-dimensional data with numerous features or variables. While more features can provide richer information, high dimensionality often leads to several challenges, such as:\n",
    "\n",
    "1.**Curse of Dimensionality**: As the number of dimensions increases, the volume of the feature space grows exponentially, making the data sparser. This sparsity can degrade the performance of machine learning models, as it becomes harder to find meaningful patterns and relationships between features.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"../images/Curse_of_Dimensionality_Chart.png\" alt=\"Curse of Dimensionality\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "2.**Increased Computational Costs**: High-dimensional data requires more memory and computational power for processing, training, and evaluation of machine learning models. This can lead to slower runtimes and higher resource consumption.\n",
    "\n",
    "3.**Overfitting**: When a model has too many features relative to the number of observations, it risks overfitting, capturing noise in the data rather than the underlying patterns. Dimensionality reduction helps mitigate overfitting by focusing on the most informative features.\n",
    "\n",
    "4.**Visualization and Interpretation**: It is difficult to visualize and interpret data in high dimensions. Dimensionality reduction techniques enable the projection of data into 2D or 3D spaces, making it easier to understand and analyze the data visually.\n",
    "\n",
    "By reducing the number of dimensions, we aim to maintain the most important information while simplifying the dataset, leading to faster computations, improved model performance, and more interpretable results."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "51a00526f29c0659"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.Principal Component Analysis (PCA)\n",
    "#### How PCA work?\n",
    "**principal component analysis (PCA)**, an unsupervised linear transformation technique that is widely used across different fields, most prominently for feature extraction and dimensionality reduction. Other popular applications of PCA include exploratory data analysis and the denoising of signals in stock market trading, and the analysis of genome data and gene expression levels in the field of bioinformatics.\n",
    "\n",
    "- **PCA** helps us to identify patterns in data based on the **correlation** between features.\n",
    "- aims to find the directions of **maximum variance** in high-dimensional data and projects the data onto a new subspace with equal or fewer dimensions than the original one.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"../images/PCA_01.png\" alt=\"Using PCA to find the directions of maximum variance in a dataset\" height=\"500\" />\n",
    "</div>\n",
    "\n",
    "In top figure **x1** and **x2** are the original feature axes, and **PC1** and **PC2** are the principal components.\n",
    "\n",
    "\n",
    "If we use PCA for dimensionality reduction, we construct a d×k-dimensional transformation matrix, W, that allows us to map a vector of the features of the training example, x, onto a new k-dimensional feature subspace that has fewer dimensions than the original d-dimensional feature space. For instance, the process is as follows. Suppose we have a feature vector, x:\n",
    "$$\\mathbf{x = \\left [ x_{1}, x_{2}, x_{3}, ..., x_{d}  \\right ] x \\epsilon \\mathbb{R^{d}}}\\boldsymbol{}$$\n",
    "\n",
    "which is then transformed by a transformation matrix: $\\mathbf{\\boldsymbol{W\\epsilon\\mathbb{R^{d*k}}}}$\n",
    "$$\\mathbf{\\boldsymbol{xW=z}}$$\n",
    "resulting in the output vector:\n",
    "$$\\mathbf{z = \\left [ z_{1}, z_{2}, z_{3}, ..., z_{k}  \\right ] z \\epsilon \\mathbb{R^{k}}}\\boldsymbol{}$$\n",
    "\n",
    "As a result of transforming the original d-dimensional data onto this new k-dimensional subspace\n",
    "(typically k << d), the first principal component will have the largest possible variance.\n",
    "\n",
    "\n",
    "let's see PCA in simple steps:\n",
    "1. Standardize the d-dimensional dataset.\n",
    "2. Construct the covariance matrix.\n",
    "3. Decompose the covariance matrix into its eigenvalues and eigenvectors.\n",
    "4. Sort the eigenvalues by decreasing order to rank the corresponding eigenvectors.\n",
    "5. Select k eigenvectors, which correspond to the k largest eigenvalues, where k is the dimension ality of the new feature subspace.$$k \\leq d$$\n",
    "6. Construct a projection matrix, **W**, from the “top” k eigenvectors.\n",
    "7. Transform the d-dimensional input dataset, **X**, using the projection matrix, **W**, to obtain the new k-dimensional feature subspace"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5866f089da90bcb2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Extracting the principal components step by step"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83528dbfbeac3544"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "   Class label  Alcohol  Malic acid   Ash  Alcalinity of ash  Magnesium  \\\n0            1    14.23        1.71  2.43               15.6        127   \n1            1    13.20        1.78  2.14               11.2        100   \n2            1    13.16        2.36  2.67               18.6        101   \n3            1    14.37        1.95  2.50               16.8        113   \n4            1    13.24        2.59  2.87               21.0        118   \n\n   Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n0           2.80        3.06                  0.28             2.29   \n1           2.65        2.76                  0.26             1.28   \n2           2.80        3.24                  0.30             2.81   \n3           3.85        3.49                  0.24             2.18   \n4           2.80        2.69                  0.39             1.82   \n\n   Color intensity   Hue  OD280/OD315 of diluted wines  Proline  \n0             5.64  1.04                          3.92     1065  \n1             4.38  1.05                          3.40     1050  \n2             5.68  1.03                          3.17     1185  \n3             7.80  0.86                          3.45     1480  \n4             4.32  1.04                          2.93      735  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Class label</th>\n      <th>Alcohol</th>\n      <th>Malic acid</th>\n      <th>Ash</th>\n      <th>Alcalinity of ash</th>\n      <th>Magnesium</th>\n      <th>Total phenols</th>\n      <th>Flavanoids</th>\n      <th>Nonflavanoid phenols</th>\n      <th>Proanthocyanins</th>\n      <th>Color intensity</th>\n      <th>Hue</th>\n      <th>OD280/OD315 of diluted wines</th>\n      <th>Proline</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>14.23</td>\n      <td>1.71</td>\n      <td>2.43</td>\n      <td>15.6</td>\n      <td>127</td>\n      <td>2.80</td>\n      <td>3.06</td>\n      <td>0.28</td>\n      <td>2.29</td>\n      <td>5.64</td>\n      <td>1.04</td>\n      <td>3.92</td>\n      <td>1065</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>13.20</td>\n      <td>1.78</td>\n      <td>2.14</td>\n      <td>11.2</td>\n      <td>100</td>\n      <td>2.65</td>\n      <td>2.76</td>\n      <td>0.26</td>\n      <td>1.28</td>\n      <td>4.38</td>\n      <td>1.05</td>\n      <td>3.40</td>\n      <td>1050</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>13.16</td>\n      <td>2.36</td>\n      <td>2.67</td>\n      <td>18.6</td>\n      <td>101</td>\n      <td>2.80</td>\n      <td>3.24</td>\n      <td>0.30</td>\n      <td>2.81</td>\n      <td>5.68</td>\n      <td>1.03</td>\n      <td>3.17</td>\n      <td>1185</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>14.37</td>\n      <td>1.95</td>\n      <td>2.50</td>\n      <td>16.8</td>\n      <td>113</td>\n      <td>3.85</td>\n      <td>3.49</td>\n      <td>0.24</td>\n      <td>2.18</td>\n      <td>7.80</td>\n      <td>0.86</td>\n      <td>3.45</td>\n      <td>1480</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>13.24</td>\n      <td>2.59</td>\n      <td>2.87</td>\n      <td>21.0</td>\n      <td>118</td>\n      <td>2.80</td>\n      <td>2.69</td>\n      <td>0.39</td>\n      <td>1.82</td>\n      <td>4.32</td>\n      <td>1.04</td>\n      <td>2.93</td>\n      <td>735</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_wine = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\",\n",
    "                      header = None)\n",
    "df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',\n",
    "                   'Alcalinity of ash', 'Magnesium', 'Total phenols',\n",
    "                   'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',\n",
    "                   'Color intensity', 'Hue',\n",
    "                   'OD280/OD315 of diluted wines', 'Proline']\n",
    "\n",
    "df_wine.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-20T08:40:49.782884405Z",
     "start_time": "2024-10-20T08:40:48.653679025Z"
    }
   },
   "id": "aa90173006f0c4b1",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values\n",
    "\n",
    "#Split data into train and test sets. 70% for train and 30% for test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify = y, random_state=0)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-20T08:40:49.812019505Z",
     "start_time": "2024-10-20T08:40:49.729672301Z"
    }
   },
   "id": "11267151c12db352",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "Standardize the data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2859c5b3234bfe8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train_std = sc.fit_transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-20T08:40:50.073343696Z",
     "start_time": "2024-10-20T08:40:49.770460681Z"
    }
   },
   "id": "27623065af70665b",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Calculate Covariance matrix**:\n",
    "the covariance between two features, $\\mathbf{\\boldsymbol{x_{j}}}$ and $\\mathbf{\\boldsymbol{x_{k}}}$, on the population level can be calculated via the following equation:\n",
    "\n",
    "$$\\mathbf{\\boldsymbol{\\sigma_{jk}=\\frac{1}{n-1}\\sum_{i=1}^{n}(x_{j}^{(i)}-\\mu_{j})(x_{k}^{(i)}-\\mu_{k})}}$$\n",
    "Here, $\\mathbf{\\boldsymbol{\\mu _{j}}}$ and $\\mathbf{\\boldsymbol{\\mu _{k}}}$ are the samples means of features j and k.\n",
    "For example, the covariance matrix of three features can then be written as follows:\n",
    "\n",
    "$$\\mathbf{\\boldsymbol{\\Sigma=\\begin{bmatrix}\\sigma _{1}^{2}&\\sigma _{12}&\\sigma _{13}\\\\\\sigma _{21}&\\sigma _{2}^{2}&\\sigma _{23}\\\\\\sigma _{31}&\\sigma _{32}&\\sigma _{3}^{2}\\\\\\end{bmatrix}}}$$\n",
    "\n",
    "\n",
    "We know that an eigenvector, $\\mathbf{\\boldsymbol{\\vartheta}}$, satisfies the following condition.\n",
    "$$\\mathbf{\\boldsymbol{\\Sigma\\vartheta=\\lambda\\vartheta}}$$\n",
    "Here, $\\mathbf{\\boldsymbol{\\lambda}}$ is scaler: the eigenvalue."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b418be523f6bae50"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
